{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://gitlab.com/ibm/skills-network/courses/placeholder101/-/raw/master/labs/module%201/images/IDSNlogo.png\" width=\"300\" alt=\"cognitiveclass.ai logo\"  />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Weather Classification Assignment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **60** minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will practice using all the classification algorithms and metrics that we learned in this course. Using weather data we will try to predict if there is going to rain the next day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this lab you will be able to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data\n",
    "    * Describe and Define the Dataset\n",
    "    * Load a CSV Dataset using Pandas\n",
    "    * Preprocess the Data using Pandas\n",
    "    * Deal with NULL Values in your Dataset\n",
    "    * Perform One Hot Encoding on Categorical Variables\n",
    "    * Split your Data into a Training and Testing Set\n",
    "    * Standardize your Data using StandardScaler or MinMax\n",
    "* Classification\n",
    "    * Use GridSearchCV to Find the Best Parameters for a Classification Algorithm\n",
    "    * Perform Classification using Logistic Regression\n",
    "    * Perform Classification using K-Nearest Neighbors\n",
    "    * Perform Classification using Support Vector Machine\n",
    "    * Perform Classification using Decision Trees\n",
    "* Use Evaluation Metrics Accuracy Score, Jaccard Index, F1-Score, and Log Loss on Each Algorithm and Report the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will download the data that we will use in this lab which is stored in a CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O weatherAUS.csv https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/ML0101ENv3/project_EdX/weatherAUS.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we are going to be using Python and several Python libraries. Some of these libraries might be installed in your lab environment or in SN Labs. Others may need to be installed by you. The cells below will install these libraries when executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas\n",
    "#pip install sklearn\n",
    "#pip install matplotlib\n",
    "#pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows us to interact with the data using a dataframe\n",
    "import pandas as pd\n",
    "# allows us to interact with the data and perform calculations using ndarrays\n",
    "import numpy as np\n",
    "# various classification algorithms and metrics from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# matplotlib allows us to create graphs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since sklearn calculates jaccard index differently than what was taught in the course we will define our own function for jaccard index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works like sklearn classificaton metrics given list or ndarray of predictions and values returns the jaccar index\n",
    "def jaccard_index(predictions, true):\n",
    "    if (len(predictions) == len(true)):\n",
    "        intersect = 0;\n",
    "        for x,y in zip(predictions, true):\n",
    "            if (x == y):\n",
    "                intersect += 1\n",
    "        return intersect / (len(predictions) + len(true) - intersect)\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original source of the data is Austrailian Government's Bureau of Meteorology and the latest data can be gathered from http://www.bom.gov.au/climate/dwo/.\n",
    "\n",
    "The dataset we will use has extra columns like RainToday and our target RainTomorrow which was gathered from Rattle at https://bitbucket.org/kayontoga/rattle/src/master/data/weatherAUS.RData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is observations of weather metrics for each day from 2008 to 2017. The __weatherAUS.csv__ dataset includes the following fields:\n",
    "\n",
    "| Field          | Description                                             | Unit            | Type   |\n",
    "|----------------|---------------------------------------------------------|-----------------|--------|\n",
    "| Date           | Date of the Observation in YYYY-MM-DD                   | Date            | object |             \n",
    "| Location       | Location of the Observation                             | Location        | object |\n",
    "| MinTemp        | Minimum temperature                                     | Celsius         | float  |     \n",
    "| MaxTemp        | Maximum temperature                                     | Celsius         | float  |\n",
    "| Rainfall       | Amount of rainfall                                      | Millimeters     | float  |\n",
    "| Evaporation    | Amount of evaporation                                   | Millimeters     | float  |\n",
    "| Sunshine       | Amount of bright sunshine                               | hours           | float  |                  \n",
    "| WindGustDir    | Direction of the strongest gust                         | Compass Points  | object |\n",
    "| WindGustSpeed  | Speed of the strongest gust                             | Kilometers/Hour | object |\n",
    "| WindDir9am     | Wind direction averaged 10 minutes prior to 9am      | Compass Points  | object |\n",
    "| WindDir3pm     | Wind direction averaged 10 minutes prior to 3pm      | Compass Points  | object |\n",
    "| WindSpeed9am   | Wind speed averaged 10 minutes prior to 9am          | Kilometers/Hour | float  |\n",
    "| WindSpeed3pm   | Wind speed averaged 10 minutes prior to 3pm          | Kilometers/Hour | float  |\n",
    "| Humidity9am    | Humidity at 9am                                         | Percent         | float  |\n",
    "| Humidity3pm    | Humidity at 3pm                                         | Percent         | float  |\n",
    "| Pressure9am    | Atmospheric pressure reduced to mean sea level at 9am   | Hectopascal     | float  |\n",
    "| Pressure3pm    | Atmospheric pressure reduced to mean sea level at 3pm   | Hectopascal     | float  |\n",
    "| Cloud9am       | Fraction of the sky obscured by cloud at 9am            | Eights          | float  |\n",
    "| Cloud3pm       | Fraction of the sky obscured by cloud at 3pm            | Eights          | float  |\n",
    "| Temp9am        | Temperature at 9am                                      | Celsius         | float  |\n",
    "| Temp3pm        | Temperature at 3pm                                      | Celsius         | float  |\n",
    "| RainToday      | If there was rain today                                 | Yes/No          | object |\n",
    "| RISK_MM        | Amount of rain tomorrow                                 | Millimeters     | float  |\n",
    "| RainTomorrow   | If there is rain tomorrow                               | Yes/No          | float  |\n",
    "\n",
    "\n",
    "Column definitions were gathered from http://www.bom.gov.au/climate/dwo/IDCJDW0000.shtml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use the __head()__ function to see our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('weatherAUS.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to focus specifically on Sydney so that we can train our algorithm quickly. You can select other locations or multiple locations if you would like to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Location'] == 'Sydney']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we drop all the columns in the table that we won't need.\n",
    "\n",
    "We drop Location because it is constant for each row and we drop RIS_MM because this tells us the amount of rain tomorrow so we can not train on it as it reveals the target and we are doing classification, not regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sydney = df[df['Location'] == 'Sydney']\n",
    "\n",
    "df_sydney.drop(columns=['Location', 'RISK_MM'], axis=1, inplace=True)\n",
    "\n",
    "print(df_sydney.shape)\n",
    "\n",
    "df_sydney.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above we have NaN occur a couple of times in our dataset. We can either drop the data or replace the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see how many NaN values we have for each row. WindGustDir, WindGustSpeed, Cloud9am, and Cloud3pm have large values of missing data. In this case for ~33% of the data, we are missing a value for WindGusDir and WindGustSpeed. This is not enough to remove the entire column but we will perform some preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sydney.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing With Nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please uncomment the method that you would like to use\n",
    "\n",
    "1. Drop all rows that contain NaN\n",
    "2. Replace NaN in object type columns like WindGustDir with most frequent value in the column and replace NaN in float type columns like WindGustSpeed, Cloud9am, and Cloud3pm with the mean. Then we drop the remaining rows with NaN in them.\n",
    "\n",
    "Please note that if you choose to replace the NaN values the classification algorithms will take a little longer to compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sydney_filled = df_sydney.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sydney_filled = df_sydney.copy()\n",
    "\n",
    "#most_frequent_WindGustDir = df_sydney_filled['WindGustDir'].value_counts().idxmax()\n",
    "#df_sydney_filled[\"WindGustDir\"].replace(np.nan, most_frequent_WindGustDir, inplace=True)\n",
    "\n",
    "#mean_WindGustSpeed = df_sydney_filled[\"WindGustSpeed\"].astype(\"float\").mean(axis=0)\n",
    "#df_sydney_filled[\"WindGustSpeed\"].replace(np.nan, mean_WindGustSpeed, inplace=True)\n",
    "\n",
    "#mean_Cloud9am = df_sydney_filled[\"Cloud9am\"].astype(\"float\").mean(axis=0)\n",
    "#df_sydney_filled[\"Cloud9am\"].replace(np.nan, mean_Cloud9am, inplace=True)\n",
    "\n",
    "#mean_Cloud3pm = df_sydney_filled[\"Cloud3pm\"].astype(\"float\").mean(axis=0)\n",
    "#df_sydney_filled[\"Cloud3pm\"].replace(np.nan, mean_Cloud3pm, inplace=True)\n",
    "\n",
    "#df_sydney_filled.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_sydney_filled.shape)\n",
    "df_sydney_filled.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have completely removed all NaN values using different methods which allow you to either remove rows with NaN in them improving the pureness of our dataset or filling in NaN values allowing us to preserve rows. When deciding on the method to use there are many benefits and drawbacks we must consider like whether or not we will have enough data after dropping NaN rows or if filling in Nan by frequency or mean will introduce some sort of bias to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sydney_filled.loc[:,'Date'] = df['Date'].str.replace('-', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we remove the - between the values of the Date column so they can be converted to floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to perform one hot encoding to convert categorical variables to binary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sydney_processed = pd.get_dummies(data=df_sydney_filled, columns=['RainToday', 'WindGustDir', 'WindDir9am', 'WindDir3pm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we replace the values of the RainTomorrow column changing it from a categorical column to a binary column. We do not use the __get_dummies__ method because we would end up with two columns for RainTomorrow and we do not want that because it is our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sydney_processed.replace(['No', 'Yes'], [0,1], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we turn all columns into a float type. We don't need to do this because the __StandardScalar()__ method will convert object types to float but it will give us a warning message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sydney_processed = df_sydney_processed.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split our dataset into a features dataset and target dataset. We drop our target to create our features dataset and only keep RainTomorrow to create our target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_sydney_processed.drop(columns='RainTomorrow', axis=1)\n",
    "Y = df_sydney_processed['RainTomorrow']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will standardize the data. We can do this in multiple ways like using the __StandardScalar()__ method which will scale the values to unit variance or the __MinMaxScalar()__ which will scale each value to the min and max of each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we standardize our data we must split it into training and testing sets. We do this before standarsizing so that we don't give any hints to out model by standardizing all the data together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(features, Y, test_size=.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please uncomment the method you would like to choose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm = preprocessing.StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = norm.fit_transform(x_train)\n",
    "\n",
    "x_test = norm.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed before you can see how we fit and the scaler to the training data and also transformed it. Then we used the fitted scaler to transform the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is where we are going to use the classification algorithms to create a model based on our training data and finally evaluate our testing data using evaluation metrics learned in the course\n",
    "\n",
    "We will some of the algorithms taught in the course, specifically \n",
    "\n",
    "1. Logistic Regression \n",
    "2. KNN\n",
    "3. SVM\n",
    "4. Decision Trees\n",
    "\n",
    "We will evaluate our models using\n",
    "\n",
    "1. Accuracy Score\n",
    "2. Jaccard Index\n",
    "3. F1-Score\n",
    "4. Log Loss\n",
    "\n",
    "Note: Jaccard Index is calculated differently in Sci Kit Learn so I have defined a function at the top of the notebook for you to use, its input style is the same as Sci Kit Learn\n",
    "\n",
    "As we know these algorithms have many parameters and to find the best ones we will use GridSearchCV\n",
    "\n",
    "I will demonstrate how to do this using a mock classification algorithm\n",
    "\n",
    "1. Create a python dictionary with the key being the name of the parameters and the value being a list of possible values\n",
    "2. Create an object of the classification algorithm\n",
    "3. Create a GridSearchCV object and place your classification object and parameters dictionary as parameters, also define your GridSearchCV cv parameter (Use cv = 4)\n",
    "4. Use the fit method of the GridSearchCV algorithm to train our model using x_train and y_train that we create before\n",
    "5. Store the best model in a variable provided\n",
    "6. Predict the target variable using the x_test data we created above\n",
    "6. Calculate and store the values for each metric in the provided variables using the predictions and y_test data\n",
    "\n",
    "You will need to research the parameters you need to use as there are many options but this is simple. GridSearchCV will determine the best model.\n",
    "\n",
    "Finally using your models generate the report at the bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "parameters = {'C': [.001, .01, .1, 1, 10, 100],\n",
    "             'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "             'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "\n",
    "Mock = MockClassification()\n",
    "\n",
    "Grid = GridSearchCV(Mock, parameters, cv = 4)\n",
    "\n",
    "Grid.fit(x_train, y_train)\n",
    "\n",
    "BestMock = Grid.best_estimator_\n",
    "\n",
    "predictions = BestMock.predict(x_test)\n",
    "\n",
    "BestMock_Accuracy_Score = accuracy_score(predictions, y_test)\n",
    "BestMock_JaccardIndex = jaccard_index(predictions, y_test)\n",
    "BestMock_F1_Score = f1_score(predictions, y_test)\n",
    "BestMock_Log_Loss = log_loss(y_test,BestLR.predict_proba(x_test)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need some more help with grid search here are a couple of resources\n",
    "1. https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
    "2. https://scikit-learn.org/stable/modules/grid_search.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Logistic Regression please use the parameters C = [.001, .01, .1, 1, 10, 100] and solver. Use the link provided to select the values for the solver parameter. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating the LogisticRegression object please make **max_iter = 10000**. This will allow us enough iteration so the model parameters can converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BestLR = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BestLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_Accuracy_Score = \n",
    "LR_JaccardIndex = \n",
    "LR_F1_Score = \n",
    "LR_Log_Loss = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For KNN please use the parameters n_neighbors = [1,2,3,4,5,6,7,8,9,10], algorithm, and p. Use the link provided to select the values for algorithm and p. https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BestKNN ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BestKNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_Accuracy_Score = \n",
    "KNN_JaccardIndex = \n",
    "KNN_F1_Score = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SVM please use the parameters C = [.001, .01, .1, 1, 10, 100] and kernel. Use the link provided to select the values for kernel. https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BestSVM ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BestSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_Accuracy_Score = \n",
    "SVM_JaccardIndex = \n",
    "SVM_F1_Score = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Decision Tree please use the parameters criterion. Use the link provided to select the values for criterion. https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BestTree ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BestTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree_Accuracy_Score = \n",
    "Tree_JaccardIndex = \n",
    "Tree_F1_Score = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Report = pd.DataFrame({'Algorithm' : ['KNN', 'Decision Tree', 'SVM', 'LogisticRegression']})\n",
    "\n",
    "Report['Accuracy'] = [LR_Accuracy_Score, KNN_Accuracy_Score, SVM_Accuracy_Score, Tree_Accuracy_Score]\n",
    "Report['Jaccard'] = [LR_JaccardIndex, KNN_JaccardIndex, SVM_JaccardIndex, Tree_JaccardIndex]\n",
    "Report['F1-Score'] = [LR_F1_Score, KNN_F1_Score, SVM_F1_Score, Tree_F1_Score]\n",
    "Report['LogLoss'] = ['N/A', 'N/A', 'N/A', LR_Log_Loss]\n",
    "\n",
    "Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Azim Hirjani](https://www.linkedin.com/in/azim-hirjani-691a07179/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2020-09-14|0.2|Azim|Update Lab to Use Template|\n",
    "|2020-04-17|0.1|Azim|Created Lab|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2020 IBM Corporation. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
